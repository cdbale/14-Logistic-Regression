---
title: "Logistic Regression"
format: docx
editor: visual
---

Load libraries.
```{r}
library(tidyverse)
library(tidymodels)
```


```{r}
set.seed(42)    # Set the randomization seed.
n <- 500        # Number of observations.
beta0 <- -1.30  # Intercept parameter.
beta1 <- 0.25   # Slope parameter for promotion spend.
beta2 <- 1.20   # Slope parameter for cold region.

# Simulate data.
sim_data <- tibble(
  x1 = runif(n, min = 0, max = 10),
  x2 = rbinom(n, size = 1, prob = 0.30),
  prob_y = exp(beta0 + beta1 * x1 + beta2 * x2) / 
    (1 + exp(beta0 + beta1 * x1 + beta2 * x2)),
  y = rbinom(n, size = 1, prob = prob_y) |> factor()
)
```

Fit our logistic regression model and output point estimates and confidence intervals.

```{r}
# Fit a logistic regression.
sim_fit <- logistic_reg() |> 
  set_engine(engine = "glm") |> 
  fit(y ~ x1 + x2, data = sim_data)

# Tidy output.
tidy(sim_fit, conf.int = TRUE)
```

Preprocess our actual data.

```{r}
# Import soup data.
soup_data <- read_csv(here::here("Data", "soup_data.csv")) |> 
  mutate(top_selling = if_else(Sales > 20000, 1, 0) |> factor())

# Split data.
soup_split <- initial_time_split(soup_data, prop = 0.90)
```

Set up recipe.

```{r}
# Prepare a preprocessing recipe.
soup_recipe <- training(soup_split) |> 
  recipe(top_selling ~ Any_Price_Decr_Spend + Retailer_Trade_Areas) |> 
  step_log(all_numeric(), offset = 1) |> 
  step_dummy(Retailer_Trade_Areas) |> 
  prep()

# Apply the recipe to the training and testing data.
soup_training <- soup_recipe |>
  bake(training(soup_split))

soup_testing <- soup_recipe |>
  bake(testing(soup_split))
```

Fit logistic regression model to the training data.

```{r}
# Fit a logistic regression.
fit <- logistic_reg() |> 
  set_engine("glm") |> 
  fit(top_selling ~ ., data = soup_training)
```


```{r}
# Compare parameter estimates.
tidy(fit, conf.int = TRUE) |> 
  ggplot(aes(y = term)) + 
  geom_point(aes(x = estimate)) + 
  geom_errorbarh(aes(xmin = conf.low, xmax = conf.high), height = .1) +
  geom_vline(xintercept = 0, color = "red")
```

Interpreting coefficients as probabilities.

```{r}
# Interpreting the parameter estimates as probabilities.
tidy(fit, conf.int = TRUE) |> 
  mutate(
    estimate = plogis(estimate),
    conf.low = plogis(conf.low),
    conf.high = plogis(conf.high)
  )
```

Generate predictions on the testing data.

```{r}
fit |> 
  predict(new_data = soup_testing, opts = list(threshold = 1))|>
  bind_cols(soup_testing) |>
  accuracy(truth = top_selling, estimate = .pred_class)
```

Look at confusion matrix.

```{r}
# Confusion matrix.
fit |> 
  predict(new_data = soup_testing) |>
  bind_cols(soup_testing) |>
  conf_mat(truth = top_selling, estimate = .pred_class)
```

```{r}
# Column names, type, and preprocessing needs to match the recipe.
scenarios <- tibble(
  # generate spend on price decreases using 4 values from 0 to 10,000
  # repeat those spend values 4 times, one repetition for each trade area
  Any_Price_Decr_Spend = seq(from = 0, to = 10000, by = 10000 / 3) |> rep(4),
  # generate the trade areas, repeat 4 times for each value of spend on price decreases
  Retailer_Trade_Areas = unique(soup_data$Retailer_Trade_Areas) |> rep(4) |> sort(),
  top_selling = factor(1)
)
```

Bake the counterfactual data so that it looks like the training data.

```{r}
# Apply the recipe to the training data.
scenarios <- soup_recipe |>
  bake(scenarios) |> 
  select(-top_selling)

scenarios
```

Generate predicted probabilities.

```{r}
# Predict success (no prediction intervals).
predict(fit, new_data = scenarios, type = "prob") |> 
  bind_cols(scenarios) |> 
  mutate(Any_Price_Decr_Spend = exp(Any_Price_Decr_Spend)) |> 
  arrange(desc(.pred_1))
```

